{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d471a71-1590-4e5a-8007-10db1bbdac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User provides: \n",
    "    ## a Python function (possibly buggy), \n",
    "    ## a short description of intended behavior, \n",
    "    ## and optionally an example unit test to use as a style/reference.\n",
    "\n",
    "# Planner: LLM (Gemini) generates unit tests for the function (using the example test if provided).\n",
    "\n",
    "# Executor: Run the tests locally with pytest and capture results (pass/fail, tracebacks).\n",
    "\n",
    "# Reflector: LLM reads test failures and suggests a corrected function.\n",
    "\n",
    "# Loop: Repeat (generate new tests / run / fix) until tests pass or max iterations reached.\n",
    "\n",
    "# Return: final (hopefully fixed) function and the test results history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "840f6d7f-d9d4-4fb6-9325-5ca555432531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.llms import Ollama\n",
    "from typing import TypedDict\n",
    "import subprocess, tempfile, os\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "# Initialize LLM\n",
    "llm = Ollama(model=\"codellama:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f3292b-b396-45ed-9bf9-af55793d2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def ask_ollama(prompt, model=\"codellama:latest\"):\n",
    "#     \"\"\"\n",
    "#     Sends a prompt to Ollama and returns the model's text response.\n",
    "#     Works with any installed Ollama model (e.g., codellama, mistral, llama3, etc.)\n",
    "#     \"\"\"\n",
    "#     url = \"http://localhost:11434/api/generate\"\n",
    "#     payload = {\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
    "#     response = requests.post(url, json=payload)\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(f\"Ollama error: {response.text}\")\n",
    "    \n",
    "#     data = response.json()\n",
    "#     return data.get(\"response\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c7feaf4-76d0-49ab-bb76-319a8423c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    func_code: str\n",
    "    description: str\n",
    "    example_test: str\n",
    "    tests: str\n",
    "    results: str\n",
    "    iteration: int\n",
    "\n",
    "\n",
    "def run_tests_and_capture(test_code, func_code):\n",
    "    \"\"\"Run pytest and capture results, safely dedented.\"\"\"\n",
    "    import textwrap\n",
    "    import re\n",
    "\n",
    "    # Remove Markdown-style ``` and any language hint\n",
    "    test_code = re.sub(r\"^```.*\\n|```$\", \"\", test_code.strip(), flags=re.MULTILINE)\n",
    "    \n",
    "    func_code = textwrap.dedent(func_code)\n",
    "    test_code = textwrap.dedent(test_code)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        func_path = os.path.join(tmpdir, \"target.py\")\n",
    "        test_path = os.path.join(tmpdir, \"test_target.py\")\n",
    "\n",
    "        with open(func_path, \"w\") as f:\n",
    "            f.write(func_code)\n",
    "        with open(test_path, \"w\") as f:\n",
    "            f.write(test_code)\n",
    "\n",
    "        result = subprocess.run(\n",
    "            [\"pytest\", \"-q\", test_path],\n",
    "            cwd=tmpdir,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            timeout=10\n",
    "        )\n",
    "\n",
    "        return result.stdout + \"\\n\" + result.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ae3960-781e-47a5-811c-4b4143c90fdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def planner_node(state: AgentState):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful Python unit test generator.\n",
    "    Generate **additional pytest unit tests** for the function.\n",
    "    Follow the style and format of the example test.\n",
    "    Take into account the Description to understand the intent of the function.\n",
    "    Follow the style, structure, and tone of this example test.\n",
    "    \n",
    "    --- Example Test ---\n",
    "    {state['example_test']}\n",
    "    ---------------------\n",
    "    \n",
    "    Description:\n",
    "    {state['description']}\n",
    "    \n",
    "    Function:\n",
    "    {state['func_code']}\n",
    "    \n",
    "    Return **ONLY** valid pytest code, **NO EXPLANATION, NO MARKDOWN, NO TEXT EXPLANATION**\n",
    "    \"\"\"\n",
    "    raw_generated = llm.invoke(prompt)\n",
    "\n",
    "    # Remove any lines that are not valid Python (skip everything before first \"def \")\n",
    "    lines = raw_generated.splitlines()\n",
    "    start_idx = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"def \"):\n",
    "            start_idx = i\n",
    "            break\n",
    "    cleaned_generated = \"\\n\".join(lines[start_idx:])\n",
    "    cleaned_code = re.sub(r\"```.*$\", \"\", cleaned_generated.strip(), flags=re.MULTILINE)\n",
    "\n",
    "    # Combine example tests + cleaned generated tests\n",
    "    state[\"tests\"] = state[\"example_test\"] + \"\\n\\n\" + cleaned_generated\n",
    "\n",
    "    print(f\"\\n--- Iteration {state['iteration']} Planner Generated Tests ---\\n\")\n",
    "    print(cleaned_generated)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d649fa42-501d-45f8-8455-f9c291e27a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executor_node(state: AgentState):\n",
    "    results = run_tests_and_capture(state[\"tests\"], state[\"func_code\"])\n",
    "    state[\"results\"] = results\n",
    "    print(f\"\\n--- Iteration {state['iteration']} Test Results ---\\n\")\n",
    "    print(state[\"results\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e6d0d7f-d84f-41e8-9d01-220d992a1eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflector_node(state: AgentState):\n",
    "    prompt = f\"\"\"\n",
    "    The following function failed some tests.\n",
    "    Please correct it based on the pytest output below.\n",
    "    Preserve the function name and signature.\n",
    "    Import or Install packages if corre\n",
    "    \n",
    "    Function:\n",
    "    {state['func_code']}\n",
    "    \n",
    "    Pytest Results:\n",
    "    {state['results']}\n",
    "    \n",
    "    Return **ONLY** corrected Python code, **NO EXPLANATION, NO MARKDOWN, NO TEXT EXPLANATION**.\n",
    "    \"\"\"\n",
    "    raw_code = llm.invoke(prompt)\n",
    "\n",
    "    # Remove Markdown or explanation lines before the function\n",
    "    # Keep only the first \"def ...\" and everything after\n",
    "    lines = raw_code.splitlines()\n",
    "    start_idx = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"def \"):\n",
    "            start_idx = i\n",
    "            break\n",
    "\n",
    "    # Optionally, stop at the next 'def ' to remove duplicates\n",
    "    end_idx = len(lines)\n",
    "    for i in range(start_idx + 1, len(lines)):\n",
    "        if lines[i].strip().startswith(\"def \"):\n",
    "            end_idx = i\n",
    "            break\n",
    "            \n",
    "    cleaned_code = \"\\n\".join(lines[start_idx:end_idx])\n",
    "    cleaned_code = re.sub(r\"```.*$\", \"\", cleaned_code.strip(), flags=re.MULTILINE)\n",
    "    \n",
    "    state[\"func_code\"] = cleaned_code\n",
    "    state[\"iteration\"] += 1\n",
    "\n",
    "    print(f\"\\n--- Iteration {state['iteration']} Corrected Function ---\\n\")\n",
    "    print(state[\"func_code\"])\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271b2682-f2d3-4479-b56a-f458c50292e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    results = state.get(\"results\") or \"\"\n",
    "    if \"failed\" in results.lower() and state.get(\"iteration\", 0) < 10:\n",
    "        return \"planner\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9564894-f79e-4f02-af39-d896bee10113",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"planner\", planner_node)\n",
    "graph.add_node(\"executor\", executor_node)\n",
    "graph.add_node(\"reflector\", reflector_node)\n",
    "\n",
    "graph.add_edge(\"planner\", \"executor\")\n",
    "graph.add_edge(\"executor\", \"reflector\")\n",
    "graph.add_conditional_edges(\"reflector\", should_continue)\n",
    "graph.set_entry_point(\"planner\")\n",
    "compiled_graph = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "882f936e-ff70-47cd-8b10-6da5a36ad45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_function_code_ = \"\"\"\n",
    "\n",
    "\n",
    "def is_prime(n):\n",
    "    \n",
    "    if n <= 1:\n",
    "        return True # Correct for 0, negative, and 1\n",
    "    \n",
    "    \n",
    "    for i in range(2, n):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\"\"\"\n",
    "\n",
    "input_function_description = \"To return True if n is a prime number, and False otherwise.\"\n",
    "\n",
    "example_test = \"\"\"\n",
    "import pytest\n",
    "from target import is_prime\n",
    "\n",
    "def test_simple_is_prime():\n",
    "    assert is_prime(17) == True\n",
    "\n",
    "\n",
    "def test_edge_cases_is_prime():\n",
    "    # 0, 1 and negative numbers are not prime\n",
    "    assert is_prime(0) == False\n",
    "\n",
    "\n",
    "def test_composite_numbers():\n",
    "    assert is_prime(4) == False\n",
    "\n",
    "\n",
    "def test_strings_and_invalid_inputs():\n",
    "    # For strings, function should ideally raise an exception\n",
    "    with pytest.raises(TypeError):\n",
    "        is_prime(\"abc\")\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "            \n",
    "initial_state = {\n",
    "    \"func_code\": input_function_code_,\n",
    "    \"description\": input_function_description,\n",
    "    \"example_test\": example_test,\n",
    "    \"tests\": \"\",\n",
    "    \"results\": \"\",\n",
    "    \"iteration\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769e638-000d-4c1e-97b8-65280a1a45e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 0 Planner Generated Tests ---\n",
      "\n",
      "def test_simple_is_prime():\n",
      "    assert is_prime(17) == True\n",
      "\n",
      "\n",
      "def test_edge_cases_is_prime():\n",
      "    # 0, 1 and negative numbers are not prime\n",
      "    assert is_prime(0) == False\n",
      "    \n",
      "\n",
      "def test_composite_numbers():\n",
      "    assert is_prime(4) == False\n",
      "    \n",
      "\n",
      "def test_strings_and_invalid_inputs():\n",
      "    # For strings, function should ideally raise an exception\n",
      "    with pytest.raises(TypeError):\n",
      "        is_prime(\"abc\")\n",
      "\n",
      "--- Iteration 0 Test Results ---\n",
      "\n",
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                     [100%]\u001b[0m\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ test_edge_cases_is_prime ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_edge_cases_is_prime\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# 0, 1 and negative numbers are not prime\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m is_prime(\u001b[94m0\u001b[39;49;00m) == \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert True == False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where True = is_prime(0)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_target.py\u001b[0m:30: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_target.py::\u001b[1mtest_edge_cases_is_prime\u001b[0m - assert True == False\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.25s\u001b[0m\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_state = compiled_graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6273bde-b7b6-4f1b-a11a-d2929d1428f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "print(\" Input Function:\\n\")\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "print(input_function_code_)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "print(\" Final Function:\\n\")\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "print(final_state[\"func_code\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n Test Results:\\n\")\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "print(final_state[\"results\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49ddca-c04c-45c4-aaf4-56d2d7b041ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fdbc4-cc55-4abf-a158-d4582902ebc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01cb24-fe22-4cbd-9872-635b50a5c7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d613a3c-cf42-460f-a7a9-e9337441aad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
