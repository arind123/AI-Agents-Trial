{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8d6019-98c2-4626-b643-c18e5cfc24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Standard Library Imports\n",
    "# ---------------------------\n",
    "import uuid                # Generate unique identifiers for objects, sessions, or messages\n",
    "import sqlite3             # Interact with SQLite databases for lightweight persistent storage\n",
    "from typing import TypedDict, List, Optional  \n",
    "                           # Type hinting: TypedDict for structured dicts, List/Optional for clearer function signatures\n",
    "import dateparser          # Needed to parse published dates\n",
    "from urllib.parse import quote_plus\n",
    "from typing import List, Dict, Optional\n",
    "import datetime # Assuming you need this for the sorting key\n",
    "\n",
    "# ---------------------------\n",
    "# Third-Party / External Libraries\n",
    "# ---------------------------\n",
    "import feedparser           # Parse RSS/Atom feeds, used here to fetch news from sources like Bing, Google, Reuters\n",
    "import gradio as gr         # Create web-based UI interfaces for chatbot interaction or demos\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# LangGraph / LangChain Imports\n",
    "# ---------------------------\n",
    "from langgraph.graph import StateGraph, END  \n",
    "                           # StateGraph: build and manage a directed graph of tasks/nodes\n",
    "                           # END: marker for terminal nodes in a workflow\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver  \n",
    "                           # SqliteSaver: checkpoint agent state to a SQLite database for persistence\n",
    "\n",
    "from langchain_ollama import OllamaLLM  \n",
    "                           # OllamaLLM: interface to local LLMs (like Mistral) via Ollama API\n",
    "from langchain_community.vectorstores import Chroma  \n",
    "                           # Chroma: vector database for storing and retrieving embeddings efficiently\n",
    "from langchain_community.embeddings import OllamaEmbeddings  \n",
    "                           # Generate embeddings for text using local Ollama models\n",
    "\n",
    "from langchain.schema import Document  \n",
    "                           # Document: standard format for text + metadata, used in vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45860a02-5efa-4f01-a913-3cb832aaadda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models and vector store...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Setting up Local LLM and Embedding Models\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Print a message to indicate the setup process has started.\n",
    "# Useful for debugging or tracking progress in longer scripts.\n",
    "print(\"Setting up models and vector store...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8041ec-6a73-438c-8521-635203e7bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arind\\AppData\\Local\\Temp\\ipykernel_25840\\1920952701.py:11: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"mistral\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the local LLM using Ollama.\n",
    "# OllamaLLM acts as the interface to a local large language model (Mistral in this case).\n",
    "# This model will be used for generating text, summarization, and other reasoning tasks\n",
    "# in your Agentic AI workflow.\n",
    "llm = OllamaLLM(model=\"mistral\")\n",
    "\n",
    "# Initialize the embedding model using OllamaEmbeddings.\n",
    "# Embeddings are vector representations of text that allow semantic similarity comparisons.\n",
    "# This is critical for Agentic AI memory retrieval and vector-based search,\n",
    "# e.g., finding relevant past documents or summaries.\n",
    "embedding_model = OllamaEmbeddings(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2db3196-adeb-4270-9d0a-91a5a625e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arind\\AppData\\Local\\Temp\\ipykernel_25840\\4200076568.py:8: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Initialize ChromaDB Vector Store for Long-Term Memory\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Create a vector store using Chroma.\n",
    "# A vector store allows storing, retrieving, and searching text embeddings efficiently.\n",
    "# This acts as the agent's long-term memory, letting it recall past summaries or relevant documents.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"news_memory\",       # Name of the collection in the vector DB; can hold multiple topics\n",
    "    embedding_function=embedding_model,  # The embedding model used to convert text into vectors\n",
    "    persist_directory=\"./news_db\"        # Directory on disk to persist the database so memory is retained across sessions\n",
    ")\n",
    "\n",
    "# Print a message indicating that the model and vector store setup is complete.\n",
    "# Useful for tracking progress when initializing Agentic AI workflows.\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52cb0ab-ae2b-4891-82b0-1e3dc348b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Function: Fetch Latest News with Topic Filtering\n",
    "# ---------------------------\n",
    "def get_latest_news(topic: str, max_articles: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches the latest, most relevant news articles from a selection of RSS feeds.\n",
    "\n",
    "    The function queries topic-specific feeds (Google, Bing) and filters generic\n",
    "    feeds (Reuters, Guardian) to find articles matching the provided topic.\n",
    "    All retrieved articles are sorted by publication date to ensure recency.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic : str\n",
    "        The primary topic or keyword to search for in news feeds (e.g., 'AI in healthcare'). \n",
    "        This is URL-encoded for safety.\n",
    "    max_articles : int, optional\n",
    "        The maximum number of finalized, valid articles to return after fetching and sorting. \n",
    "        Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        A list of dictionaries, where each dictionary represents a single article \n",
    "        with the following structure:\n",
    "        \n",
    "        * **'title'** (str): The headline of the article.\n",
    "        * **'link'** (str): The URL to the original article source.\n",
    "        * **'published'** (str): The raw publication date string from the RSS feed.\n",
    "        * **'published_dt'** (datetime.datetime): The parsed datetime object, used for sorting.\n",
    "        * **'source'** (str): The human-readable name of the news source (e.g., 'Google', 'Bing').\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    1.  The `topic` is case-insensitively matched against the article title and summary/description.\n",
    "    2.  Articles without a valid publication date are filtered out to ensure accurate sorting.\n",
    "    3.  A placeholder generic feed is used for 'Reuters' and 'Guardian' since they \n",
    "        do not easily support arbitrary topic searching via standard RSS.\n",
    "    \"\"\"\n",
    "\n",
    "    articles = []\n",
    "    \n",
    "    # 1. URL ENCODE THE TOPIC ONCE\n",
    "    # This replaces spaces with '+' and handles other special characters\n",
    "    ENCODED_TOPIC = quote_plus(topic) \n",
    "    ENCODED_TOPIC_WITH_RECENCY = quote_plus(f\"{topic} when:1d\")\n",
    "\n",
    "    # 2. DEFINE RSS SOURCES USING THE ENCODED TOPIC\n",
    "    # Bing and Google support topic-specific search in RSS; Reuters and Guardian are generic feeds.\n",
    "    RSS_SOURCES = {\n",
    "        # Use ENCODED_TOPIC for search-based RSS feeds\n",
    "        \"Bing\": f\"https://www.bing.com/news/search?q={ENCODED_TOPIC}&format=rss\",\n",
    "        \"Google\": f\"https://news.google.com/rss/search?q={ENCODED_TOPIC_WITH_RECENCY}&hl=en-US&gl=US&ceid=US:en\",\n",
    "        # # Use generic high-level feeds for sources that don't support dynamic topic RSS\n",
    "        # \"Reuters\": \"https://www.reutersagency.com/feed/?best-topics=top-news\",\n",
    "        # \"Guardian\": \"https://www.theguardian.com/world/rss\"\n",
    "    }\n",
    "\n",
    "    # Ensure max_articles is an integer (safety for Gradio inputs)\n",
    "    try:\n",
    "        max_articles = int(max_articles)\n",
    "    except ValueError:\n",
    "        max_articles = 10 # Default to 10 if conversion fails\n",
    "        \n",
    "    topic_lower = topic.lower()\n",
    "\n",
    "    # Loop through each source name and its RSS feed URL\n",
    "    for source_name, feed_url in RSS_SOURCES.items(): \n",
    "        print(f\"Fetching news from {source_name}...\") \n",
    "        \n",
    "        # Parse the RSS feed\n",
    "        feed = feedparser.parse(feed_url)\n",
    "\n",
    "        # Loop through each article entry in the feed\n",
    "        for entry in feed.entries:\n",
    "            # Get published date if available\n",
    "            published = getattr(entry, \"published\", \"N/A\")\n",
    "            try:\n",
    "                # Use dateparser.parse\n",
    "                published_dt = dateparser.parse(published)\n",
    "            except Exception:\n",
    "                published_dt = None\n",
    "\n",
    "            # Extract article summary/description if available\n",
    "            summary = getattr(entry, \"summary\", \"\")\n",
    "\n",
    "            # Check if article matches topic (case-insensitive)\n",
    "            if topic_lower in entry.title.lower() or topic_lower in summary.lower():\n",
    "                # Append article metadata, storing only the clean source name\n",
    "                articles.append({\n",
    "                    \"title\": entry.title,\n",
    "                    \"link\": entry.link,\n",
    "                    \"published\": published,\n",
    "                    \"published_dt\": published_dt,\n",
    "                    \"source\": source_name\n",
    "                })\n",
    "\n",
    "    # Filter out articles without a valid datetime\n",
    "    valid_articles = [a for a in articles if a[\"published_dt\"] is not None]\n",
    "\n",
    "    # Sort by newest first. Use datetime.datetime.min as a fallback key for safety.\n",
    "    articles_sorted = sorted(\n",
    "        valid_articles,\n",
    "        key=lambda x: x[\"published_dt\"] if x[\"published_dt\"] is not None else datetime.datetime.min,\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Return only the top `max_articles`\n",
    "    return articles_sorted[:max_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40af665d-ca17-46b7-8358-90ca75b6cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Define the Agent State\n",
    "# ---------------------------\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the LangGraph agent as it moves between nodes.\n",
    "\n",
    "    Each node in the graph can read from and update this state. Using TypedDict\n",
    "    ensures type safety and clear expectations for each field.\n",
    "    \"\"\"\n",
    "    topic: str                          # The current news topic under discussion\n",
    "    news: Optional[List[dict]]          # List of fetched news articles (optional)\n",
    "    summary: Optional[str]              # Summarized news content (optional)\n",
    "    user_input: Optional[str]           # Latest user message (optional)\n",
    "    chat_history: List[dict]            # Conversation history in the format:\n",
    "                                        # [{\"role\": \"user/assistant\", \"content\": \"...\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51339d7-d1d4-4915-bf1f-404f99924a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# CHECKPOINTER SETUP (SQLite)\n",
    "# ---------------------------\n",
    "# Create an in-memory SQLite connection.\n",
    "# The parameter check_same_thread=False is required for web apps or multi-threaded environments\n",
    "# to allow SQLite connections to be shared across threads safely.\n",
    "conn = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
    "\n",
    "# Initialize a SqliteSaver checkpoint object using the connection.\n",
    "# This enables persistent state saving of the agent between node executions.\n",
    "# The agent can \"remember\" past summaries, conversations, or other relevant info.\n",
    "memory = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f6b420-03b8-44ac-aeef-c9f1bdab8b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Agent Node: Fetch News\n",
    "# ---------------------------\n",
    "\n",
    "def fetch_news_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node 1 in the LangGraph workflow: Fetch fresh news articles for the current topic.\n",
    "\n",
    "    This function represents a single node in the Agentic AI graph. It updates the\n",
    "    agent's state with the latest news articles fetched from RSS sources.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, which includes the topic\n",
    "                            and optionally chat history or previous news.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: Updated state with fetched news stored in `state[\"news\"]`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the current topic from the state.\n",
    "    # This topic will be used to fetch relevant news articles.\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Fetch the latest news for the topic using the helper function.\n",
    "    # This function may retrieve articles from multiple sources and return\n",
    "    # a list of dictionaries containing metadata like title, link, published date, and source.\n",
    "    articles = get_latest_news(topic)\n",
    "\n",
    "    # Store the fetched articles in the agent's state under the 'news' key.\n",
    "    state[\"news\"] = articles\n",
    "\n",
    "    # Ensure that chat history exists in the state.\n",
    "    # Some nodes downstream may expect 'chat_history' to always be present.\n",
    "    if \"chat_history\" not in state:\n",
    "        state[\"chat_history\"] = []\n",
    "\n",
    "    # Return the updated state so it can be passed to the next node in the graph.\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "816fda6f-407f-4ca5-b8a8-f10e43db6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Agent Node: Summarize & Store News\n",
    "# ---------------------------\n",
    "\n",
    "def summarize_and_store_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node 2 in the LangGraph workflow: Summarizes fetched news articles,\n",
    "    stores the summary in the vector store (long-term memory), and prepares\n",
    "    the initial assistant response for the user.\n",
    "\n",
    "    This node allows the Agentic AI to:\n",
    "    1. Create a concise summary of multiple news articles.\n",
    "    2. Store knowledge in a retrievable format (vector store).\n",
    "    3. Initialize conversation history for follow-up interactions.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including 'news' articles and 'topic'.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: Updated state with 'summary' and initialized 'chat_history'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print a message indicating that summarization is in progress\n",
    "    # Helps track execution when running the agent\n",
    "    print(\"Summarizing and storing news...\")\n",
    "\n",
    "    # Extract the current topic from the state\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Extract the list of fetched articles from the state\n",
    "    articles = state[\"news\"]\n",
    "\n",
    "    # ---------------------------\n",
    "    # Summarize Articles\n",
    "    # ---------------------------\n",
    "    if not articles:\n",
    "        # Handle case where no news articles were fetched\n",
    "        summary = \"No recent news found for this topic.\"\n",
    "    else:\n",
    "        # Combine article titles into a single string for summarization\n",
    "        combined_titles = \"\\n\".join([f\"- {a['title']}\" for a in articles])\n",
    "\n",
    "        # Construct a prompt for the LLM to generate a summary\n",
    "        # We include instructions to mention the source and published date of each article\n",
    "        prompt = f\"\"\"You are a news summarizer. \n",
    "        Briefly summarize the key points from the following news headlines about '{topic}':\\n\\n{combined_titles}. \n",
    "        Include the published date and source of each news item along with the summary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Invoke the LLM to generate the summary text\n",
    "        summary = llm.invoke(prompt)\n",
    "\n",
    "    # Store the summary in the state for downstream nodes or conversation\n",
    "    state[\"summary\"] = summary\n",
    "\n",
    "    # ---------------------------\n",
    "    # Store Summary in Vector Store (Memory)\n",
    "    # ---------------------------\n",
    "    if articles:\n",
    "        # Wrap the summary into a Document object with metadata\n",
    "        doc = Document(page_content=summary, metadata={\"topic\": topic})\n",
    "\n",
    "        # Add the document to Chroma vector store\n",
    "        # This allows semantic retrieval later, enabling memory-based reasoning\n",
    "        vectorstore.add_documents([doc])\n",
    "\n",
    "        # Persist the vector store to disk to ensure memory is not lost across sessions\n",
    "        vectorstore.persist()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Prepare Initial Assistant Response\n",
    "    # ---------------------------\n",
    "    if articles:\n",
    "        # If articles exist, build a user-friendly initial response including the summary\n",
    "        initial_response = (\n",
    "            f\"üì∞ Here are the summary of the news articles I found for **{topic}**:\\n\\n\"\n",
    "            f\"{summary}\\n\\n\"\n",
    "            \"Feel free to ask me any follow-up questions!\"\n",
    "        )\n",
    "    else:\n",
    "        # If no articles were found, the summary itself is the response\n",
    "        initial_response = summary\n",
    "\n",
    "    # Initialize chat history in the state with:\n",
    "    # - User's initial topic/message\n",
    "    # - Assistant's initial response (summary)\n",
    "    state[\"chat_history\"] = [\n",
    "        {\"role\": \"user\", \"content\": topic},\n",
    "        {\"role\": \"assistant\", \"content\": initial_response}\n",
    "    ]\n",
    "\n",
    "    # Return the updated state for the next node in the LangGraph workflow\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b68b6f-5ec9-44d7-b137-2bfa5831739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Agent Node: Continue Conversation\n",
    "# ---------------------------\n",
    "\n",
    "def conversation_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Node 3 in the LangGraph workflow: Handles follow-up conversations with the user,\n",
    "    using context from past chat history and relevant news summaries stored in memory.\n",
    "\n",
    "    This node allows the Agentic AI to:\n",
    "    1. Retrieve contextually relevant past information from memory (vector store).\n",
    "    2. Format recent conversation turns to maintain coherent multi-turn dialogue.\n",
    "    3. Use the LLM to generate a concise and context-aware reply.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including topic, user_input, and chat_history.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: Updated state with the new conversation turn appended to chat_history.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print a message to indicate this node is executing\n",
    "    print(\"Continuing conversation...\")\n",
    "\n",
    "    # Extract relevant fields from state\n",
    "    topic = state[\"topic\"]                     # Current topic for context\n",
    "    user_input = state[\"user_input\"]           # Latest message from the user\n",
    "    chat_history = state.get(\"chat_history\", [])  # Retrieve existing chat history; default to empty\n",
    "\n",
    "    # ---------------------------\n",
    "    # Retrieve Context from Memory\n",
    "    # ---------------------------\n",
    "    # Perform a semantic similarity search in the vector store using the user's input\n",
    "    # 'k=1' retrieves the most relevant past summary\n",
    "    retrieved_docs = vectorstore.similarity_search(user_input, k=1)\n",
    "\n",
    "    # Combine retrieved documents into a single string for context in the LLM prompt\n",
    "    retrieved_context = \"\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Format Recent Conversation History\n",
    "    # ---------------------------\n",
    "    # Take the last 4 turns of chat history to provide recent context\n",
    "    # This ensures the LLM has memory of the immediate conversation for coherent replies\n",
    "    context_str = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in chat_history[-4:]])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Construct Prompt for LLM\n",
    "    # ---------------------------\n",
    "    # The prompt guides the LLM to generate a concise, context-aware response\n",
    "    # It includes:\n",
    "    # 1. Current topic\n",
    "    # 2. Relevant past summaries retrieved from vector store\n",
    "    # 3. Recent conversation history\n",
    "    # 4. The user's latest question\n",
    "    prompt = f\"\"\"You are a helpful AI news assistant. Your current topic is '{topic}'.\n",
    "    Answer the user's question concisely based on the conversation history and the provided relevant news summaries.\n",
    "    \n",
    "    Relevant past summaries for context:\n",
    "    ---\n",
    "    {retrieved_context}\n",
    "    ---\n",
    "    \n",
    "    Current conversation:\n",
    "    {context_str}\n",
    "    User: {user_input}\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    # Invoke the LLM to generate a response\n",
    "    reply = llm.invoke(prompt)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Update Chat History\n",
    "    # ---------------------------\n",
    "    # Append the user's input and assistant's reply as new turns\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    # Save the updated chat history back into the agent's state\n",
    "    state[\"chat_history\"] = chat_history\n",
    "\n",
    "    # Return the updated state for the next node in the graph\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bf8048-3abf-4b67-94e9-a7881f14436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Router Function for Conditional Entry\n",
    "# ---------------------------\n",
    "\n",
    "def route_initial_or_followup(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next node in the LangGraph workflow based on the current agent state.\n",
    "\n",
    "    This function acts as a router, deciding whether the agent should:\n",
    "    1. Fetch fresh news if the state is not initialized, or\n",
    "    2. Continue a follow-up conversation if news is already present.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the next node to execute (\"fetch_news\" or \"conversation\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if 'news' key in the state is empty or None\n",
    "    # This indicates that the agent has not yet fetched any news for the topic\n",
    "    if state.get(\"news\") is None:\n",
    "        return \"fetch_news\"  # Direct the workflow to the news-fetching node\n",
    "\n",
    "    # If news is already present, the state has been initialized\n",
    "    # The next step is to handle a follow-up conversation with the user\n",
    "    else:\n",
    "        return \"conversation\"  # Direct the workflow to the conversation node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedf6f9b-5f3a-44f3-9654-d9c4dd81ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Build the Agentic AI Graph\n",
    "# ---------------------------\n",
    "\n",
    "# Initialize a new StateGraph instance using AgentState as the structure for state\n",
    "# This object will hold all nodes, edges, and the execution flow of the agent.\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# ---------------------------\n",
    "# Add All Nodes to the Graph\n",
    "# ---------------------------\n",
    "# Each node represents a discrete step in the agent's workflow\n",
    "# 1. \"fetch_news\" ‚Üí fetches news articles for the topic\n",
    "# 2. \"summarize_and_store\" ‚Üí summarizes news and stores in vector memory\n",
    "# 3. \"conversation\" ‚Üí handles follow-up questions using past summaries and chat history\n",
    "graph_builder.add_node(\"fetch_news\", fetch_news_node)\n",
    "graph_builder.add_node(\"summarize_and_store\", summarize_and_store_node)\n",
    "graph_builder.add_node(\"conversation\", conversation_node)\n",
    "\n",
    "# ---------------------------\n",
    "# Connect Nodes with Edges\n",
    "# ---------------------------\n",
    "# Defines the execution sequence between nodes\n",
    "# After fetching news, the next step is to summarize and store the articles\n",
    "graph_builder.add_edge(\"fetch_news\", \"summarize_and_store\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define Finishing Points\n",
    "# ---------------------------\n",
    "# Nodes where execution can end\n",
    "# 1. After summarizing and storing news (initial setup complete)\n",
    "# 2. After a conversation node (follow-up interaction complete)\n",
    "graph_builder.set_finish_point(\"summarize_and_store\")\n",
    "graph_builder.set_finish_point(\"conversation\")\n",
    "\n",
    "# ---------------------------\n",
    "# Set Conditional Entry Point\n",
    "# ---------------------------\n",
    "# This allows the agent to decide dynamically where to start based on state\n",
    "# Uses the router function 'route_initial_or_followup'\n",
    "# - If no news exists, start with 'fetch_news'\n",
    "# - Otherwise, start directly at 'conversation'\n",
    "graph_builder.set_conditional_entry_point(\n",
    "    route_initial_or_followup,  # Function that decides next node\n",
    "    {\n",
    "        \"fetch_news\": \"fetch_news\",\n",
    "        \"conversation\": \"conversation\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Compile the Graph into an Executable Agent\n",
    "# ---------------------------\n",
    "# Converts the defined graph, nodes, and edges into a runnable agent\n",
    "# The checkpointer (memory) allows state persistence between runs\n",
    "agent = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f98913e-767f-4446-9067-5f22618abd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Gradio Interface\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def chatbot_interface(topic, user_message, state_history):\n",
    "    \"\"\"\n",
    "    Main function to handle interaction between the user and the Agentic AI graph.\n",
    "\n",
    "    This function is designed to be used with Gradio's Chatbot interface. It:\n",
    "    1. Prepares the input state for the agent.\n",
    "    2. Invokes the agent (LangGraph) to run the workflow.\n",
    "    3. Formats the conversation into a Gradio-compatible chat history.\n",
    "\n",
    "    Args:\n",
    "        topic (str): The current news topic or context.\n",
    "        user_message (str): The latest message from the user.\n",
    "        state_history (dict): Stores persistent session info, including 'thread_id' for memory.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - Empty string to clear Gradio input box.\n",
    "            - List of (user, assistant) message tuples for the Chatbot display.\n",
    "            - Updated state_history with thread_id for persistence.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # Retrieve or create a unique thread ID\n",
    "    # ---------------------------\n",
    "    # Each conversation thread uses a unique ID to persist memory in the LangGraph checkpointer.\n",
    "    thread_id = state_history.get(\"thread_id\") or str(uuid.uuid4())\n",
    "\n",
    "    # Config object for agent invocation\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Prepare the input state for the agent\n",
    "    # ---------------------------\n",
    "    agent_input = {\n",
    "        \"topic\": topic,          # Current news topic\n",
    "        \"user_input\": user_message,  # User's latest message\n",
    "    }\n",
    "\n",
    "    # ---------------------------\n",
    "    # Invoke the Agentic AI graph\n",
    "    # ---------------------------\n",
    "    # Runs the compiled agent graph (fetch, summarize, conversation) using the current input.\n",
    "    # Returns the full updated state including chat_history and summaries.\n",
    "    full_state = agent.invoke(agent_input, config=config)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Format chat history for Gradio\n",
    "    # ---------------------------\n",
    "    gradio_chat_history = []          # Initialize an empty list for Gradio\n",
    "    chat_log = full_state.get(\"chat_history\", [])  # Extract the agent's chat history\n",
    "\n",
    "    # Convert the agent's chat log into (User, Assistant) pairs\n",
    "    i = 0\n",
    "    while i < len(chat_log):\n",
    "        user_msg = None\n",
    "        assistant_msg = None\n",
    "\n",
    "        # Capture user's message\n",
    "        if chat_log[i]['role'] == 'user':\n",
    "            user_msg = chat_log[i]['content']\n",
    "            i += 1\n",
    "\n",
    "        # Capture assistant's reply\n",
    "        if i < len(chat_log) and chat_log[i]['role'] == 'assistant':\n",
    "            assistant_msg = chat_log[i]['content']\n",
    "            i += 1\n",
    "\n",
    "        # Append the pair to Gradio chat history if both exist\n",
    "        if user_msg and assistant_msg:\n",
    "            gradio_chat_history.append((user_msg, assistant_msg))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Return values for Gradio\n",
    "    # ---------------------------\n",
    "    # 1. \"\" ‚Üí Clears the input box\n",
    "    # 2. gradio_chat_history ‚Üí Updates the chatbot display\n",
    "    # 3. {\"thread_id\": thread_id} ‚Üí Persist session info for memory\n",
    "    return \"\", gradio_chat_history, {\"thread_id\": thread_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a6fa9-600d-4e3c-9bb5-8f6825b714c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arind\\AppData\\Local\\Temp\\ipykernel_25840\\3954191100.py:35: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"NewsBot Conversation\", height=500)  # main chat display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from Bing...\n",
      "Fetching news from Google...\n",
      "Summarizing and storing news...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arind\\AppData\\Local\\Temp\\ipykernel_25840\\2023718107.py:68: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing conversation...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Gradio Interface for Conversational News AI Agent\n",
    "# ---------------------------\n",
    "\n",
    "# Use Gradio Blocks to build a structured, multi-component interface\n",
    "# Title and theme set for the interface\n",
    "with gr.Blocks(title=\"üì∞ Conversational News AI Agent with Memory\", theme=gr.themes.Default()) as iface:\n",
    "    \n",
    "    # Markdown header for the interface\n",
    "    gr.Markdown(\"# üß† News AI Agent with Memory\")\n",
    "    gr.Markdown(\n",
    "        \"Enter a topic to get News summary. \"\n",
    "        \"The agent uses **LangGraph** for flow control (fetch once, then chat) \"\n",
    "        \"and **ChromaDB** for memory.\"\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Topic input row\n",
    "    # ---------------------------\n",
    "    with gr.Row():\n",
    "        # Textbox to enter the news topic\n",
    "        # Changing the topic starts a new conversation\n",
    "        topic = gr.Textbox(\n",
    "            label=\"Enter a Topic (Changing this value starts a new conversation)\",\n",
    "            value=\"AI in healthcare\",  # default topic\n",
    "            scale=3,  # relative width in the row\n",
    "        )\n",
    "\n",
    "        # Button to trigger initial news fetch and start chat\n",
    "        start_btn = gr.Button(\"Get News & Start Chat\", scale=1, variant=\"primary\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Chat components\n",
    "    # ---------------------------\n",
    "    chatbot = gr.Chatbot(label=\"NewsBot Conversation\", height=500)  # main chat display\n",
    "    msg_box = gr.Textbox(\n",
    "        label=\"Your Message\", \n",
    "        placeholder=\"Ask a follow-up question...\", \n",
    "        interactive=True  # allow user input\n",
    "    )\n",
    "    \n",
    "    # Hidden state to store conversation history and thread ID for persistence\n",
    "    state_history = gr.State({})\n",
    "\n",
    "    # ---------------------------\n",
    "    # Interaction Logic\n",
    "    # ---------------------------\n",
    "\n",
    "    # 1Ô∏è‚É£ Initial Start: When user clicks 'Start Chat' button\n",
    "    # Calls the chatbot_interface function with the topic as both 'topic' and initial user message\n",
    "    start_btn.click(\n",
    "        chatbot_interface,\n",
    "        inputs=[topic, topic, state_history],  # topic, initial user message, persistent state\n",
    "        outputs=[msg_box, chatbot, state_history]  # update message box, chatbot display, state\n",
    "    )\n",
    "    \n",
    "    # 2Ô∏è‚É£ Follow-up Messages: When user submits a message in the msg_box\n",
    "    # Calls the chatbot_interface to continue the conversation\n",
    "    msg_box.submit(\n",
    "        chatbot_interface,\n",
    "        inputs=[topic, msg_box, state_history],  # topic, user's follow-up message, state\n",
    "        outputs=[msg_box, chatbot, state_history]  # update interface components\n",
    "    )\n",
    "\n",
    "    # 3Ô∏è‚É£ Clear Button Logic: Reset the chat and start a new topic\n",
    "    def clear_all():\n",
    "        # Returns empty message box, clears chatbot, resets state\n",
    "        return \"\", None, {} \n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat & Start New Topic\", variant=\"secondary\")\n",
    "    clear_btn.click(\n",
    "        clear_all,  # function to clear all components\n",
    "        None,  # no inputs needed\n",
    "        [msg_box, chatbot, state_history],  # outputs to reset\n",
    "        queue=False  # immediate reset without queuing\n",
    "    )\n",
    "\n",
    "# Launch the Gradio interface\n",
    "# debug=True allows live reloads and error messages in the browser console\n",
    "iface.launch(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de63e2a-122b-4684-8f6d-51438e076e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca51da-658b-4cb3-8fb5-53cf1db1f634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
