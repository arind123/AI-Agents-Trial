{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c43fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee453482",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FOLDER = \"pdfs\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 50\n",
    "VECTORSTORE_PATH = \"rag_vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1f33fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OllamaEmbeddings(model = \"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aff03825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_store_faiss_index(pdf_path):\n",
    "    \"\"\"Builds and stores FAISS index from extracted text.\"\"\"\n",
    "    print(f\" Reading: {pdf_path}\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\" Splitting text into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"ðŸ”¹ Created {len(chunks)} text chunks.\")\n",
    "\n",
    "    print(f\"Creating embeddings using {EMBED_MODEL}...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "\n",
    "    print(\"Building FAISS index...\")\n",
    "    vectorstore = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    paper_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    index_path = os.path.join(OUTPUT_DIR, paper_name)\n",
    "    vectorstore.save_local(index_path)\n",
    "\n",
    "    print(f\"FAISS index saved at: {index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be11a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading: pdfs/Attention_Is_All_You_Need.pdf\n",
      " Splitting text into chunks...\n",
      "ðŸ”¹ Created 94 text chunks.\n",
      "Creating embeddings using nomic-embed-text...\n",
      "Building FAISS index...\n",
      "FAISS index saved at: rag_indexes\\Attention_Is_All_You_Need\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    build_and_store_faiss_index(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6ca5253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are four questions based on the provided context:\n",
      "\n",
      "1. What is the purpose of performing attention functions in parallel, according to the text?\n",
      "a) To compute the dot products between queries and keys\n",
      "b) To calculate the variance of the dot product\n",
      "c) To illustrate why dot products get large\n",
      "d) To facilitate parallel computation\n",
      "\n",
      "Answer: d) To facilitate parallel computation\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "\n",
    "query = \"I have read through the paper. Prepare 4 question to quiz me.\"\n",
    "\n",
    "# Load FAISS index\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = FAISS.load_local(\"rag_indexes/attention_is_all_you_need\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Retrieve relevant context\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Ask your local LLM (e.g., llama3)\n",
    "response = ollama.chat(model=\"llama3\", messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a research assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Answer based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\"}\n",
    "])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcc7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46322eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
